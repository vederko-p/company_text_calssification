# Задача сопоставления наименований компаний

**Постановка задачи**

В качестве данных представлены пары фраз с метками, обозначающими, являются ли
фразы описаниями одной компании. Цель - построить модель, которая сможет сопоставлять
фразы и определять, относятся ли они к одной компании или нет.

[Исходные данные - full_data.csv](https://drive.google.com/drive/folders/1N-s6r0vYi_XO79_Bp8sCsm9ylFFyp3V3?usp=sharing)

# Анализ данных

Выборка представляет собой набор пар фраз и факт их соответствия. Из первичного анализа данных можно вынести то, что выборка сильно несбалансирована, у многих фраз нет совпадающих по смыслу пар. 

Изначально фразы содержат всего 6 разделимых наборов символов:
* знаки препинания и цифры
* латиница
* кириллица
* другие три набора с иероглифами
Латиница существенно преобладает на множестве пар с одинаковым языком ввиду чего было решено отбросить другие языки из датасета.

Также можно заметить, что по всем парам фраз можно построить неполносвязный граф, подграфами которого будут представлены фразы, описывающие одну и ту же компанию. Общее число уникальных фраз составило 17 571 фраз, а число уникальны классов: 16 829. Таким образом решать задачу сопоставления как задачу классификации не получится.

В конечном итоге данные в виде пар были разделены на обучающую и тестовую выборку с сохранением соотношения между классами.

# Описание решения

В качестве итоговой системы было решено реализовать систему ранжирования релевантных фраз по входящей фразе.

## Pipeline

На вход модели подается название компании, которое сравнивается с имеющимся списком компаний. 

На первом этапе идет отбор компаний из списка по вхождению слов. Перед этим каждое слово из заданного названия компании проверяется по частотному словарю, чтобы исключить слова, которые встречаются очень часто и поэтому не несут смысловой нагрузки. 

После этого все названия токенизируются при помощи предобученного токенизатора и выравниваются по длине вектора.

На втором этапе, отобранные компании из списка, попарно сравниваются с заданным при помощи сиамской нейронной сети по косинусному расстоянию между векторами. Одинаковыми компаниями считаются те, у которых косинусное расстояние больше подобранного порогового значения.

На выходе мы получаем список компаний отсортированный по косинусному расстоянию.

## Результаты обучения моделей

Оценка качества решения производилась на отложенном датасете из исходных данных. В качестве baseline модели рассматривалась модель на вхождении слов. В качестве моделей на базе глубокого обучения рассматривались различные варианты векторизации текста BERT, LSTM, полносвязные сети в контексте сиамских сетей.

Для первичной оценки качетсва работы модели использовались метрики Precision и Recall:

| model     | Parameters                      | Accuracy    |  P  | R    |
| ----------|:-------------------------------:| -----       |-----|------|
| Name comparison | TF-Idf                    |             | 0.89| 0.87 |
| Siamese Network, Bert |LSTM 128, Embeddig 768           | 0.92        | 0.82| 0.84 |
| Siamese Network       |LSTM 64 , Embeddig 100           | 0.97        | 0.94| 0.91 |
| Siamese Network       |LSTM 64 , Embeddig 50            | 0.97        | 0.95| 0.93 |
| Siamese Network       |LSTM 128,64 , Embeddig 150       | 0.97        | 0.93| 0.94 |
| Siamese Network       |LSTM 128,64 , Embeddig 50        | 0.94        | 0.84| 0.95 |
| Siamese Network       |Dense 256,128 , Embeddig 100     | 0.97        | 0.95| 0.94 |
| Siamese Network       |Dense 128 , Embeddig 100         | 0.98        | 0.96| 0.94 |
| Siamese Network       |Dense 64 , Embeddig 50           | 0.92        | 0.82| 0.86 |

Лучшая архитектура:

![модель](https://user-images.githubusercontent.com/64748758/198024052-0e990c2d-2b42-44c3-a87f-583bc054970c.png)

 **Подход к вычислению метрики ранжирования**

Цель: посчитать точность модели ранжирования с учетом специфики данных, состоящей в том, что некоторые компании описываются всего одной фразой. Таким образом помимо естественных выходных данных для сравнения: настоящий набор и предсказанный отранжированный набор, необходимо также учесть ситуации, в которых на запрос не нужно ничего выводить.

Для обработки ситуаций, когда в настоящем (GT) наборе ответов $Y_{true}$ на запрос $q\in Q$ присутствуют какие-либо фразы, используется метрика $MP$:

$$
MP(Q) = \frac{1}{|Q|}\sum_{q\in Q}{P(q)},
\quad P(q)= \frac{1}{n} \sum_{k\in \{1, \dots, n\}}{y(y_{pred}^k)},
\quad \text{where}
$$

$n=|Y_{true}|$. $y$ - истинная функция оценки релевантности ответа. $y_{pred}^k \in Y_{pred}$ - $k$-ый предсказанный моделью ответ, где набор $Y_{pred}$ упорядочен по степени релевантности, возвращаемой моделью.

В случае, когда в качестве ответа выводить нечего, метрика рассчитывается следующим образом:
$$P^*(q)=\left(\frac{1}{1+k}\right)^{m},$$

$k$ - количество ответов, в которых модель уверена. $m>1$ - нормирующая степень, так как при $m=1$ значения получаются непропорционально большими относительно Precision. В данной работе $m$ полагалось равным $3$.

Значение метрики при разном пороге

|   Threshold | 0.5 | 0.6  | 0.7 | 0.75 | 0.8 | 0.85 | 0.9 | 0.95 |
| ------------|-----|------|-----|------|-----|------|-----|------|
|    MP       |0.85 |0.87   |0.89 |0.90  |0.91 |0.92  |0.93 |0.94  |

Значение метрики увеличивается с ростом порога ввиду того, как устроена метрика.
Есть предположение связанное с тем, что метрика, работающая в случае, когда рекомендовать нечего, может
сильно перебивать обычный Precision так как таких фраз в 10 раз больше. Для этого и вводилась степень при значениях этой метрики,
однако возможно выбранная степень недостаточно прижимает показатели на указанной части выборки.

# Performance

В среднем модель обрабатывает запрос со скоростью 1 фраза / 10 секунд на наборе фраз размера 17 571 на CPU Intel Core i5-3470 3.2GHz. 

# Инструкция установки и запуска

Необходимо установить зависимости:

    pip install -r requirements.txt

После всех настроек. Из src:

    python3 test_model.py "Название компании" (название в двойных ковычках)

    python3 test_model.py "Michelin"
